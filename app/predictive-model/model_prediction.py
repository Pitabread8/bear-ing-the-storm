# -*- coding: utf-8 -*-
"""Model_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I8mffmsxmlfzBVCdMnHPQXdwt1By0XUf
"""

import pandas as pd
import numpy as np
from math import radians, cos, sin, asin, sqrt

from datetime import datetime, timedelta
from sklearn.linear_model import LinearRegression

def train_and_predict_future(df, year, month):
    """
    Fit a linear regression model for each variable in the MERRA-2 dataframe
    and predict that variable's value for a future (year, month),
    rounding predictions to 2 decimal places.
    """
    if df.empty or len(df) < 6:
        raise ValueError("Not enough historical data to train models.")

    # --- Prepare time features ---
    df_feat = df.copy()
    df_feat["year"]  = df_feat["date"].dt.year
    df_feat["month"] = df_feat["date"].dt.month
    df_feat["t"]     = np.arange(len(df_feat))  # time index

    X = df_feat[["t", "month"]].values

    predictions = {}
    models = {}

    # --- Train one model per numeric variable ---
    target_vars = [c for c in df_feat.columns if c not in ["date", "year", "month", "t"]]
    for var in target_vars:
        y = df_feat[var].values
        if np.isfinite(y).sum() < 6:
            continue
        model = LinearRegression()
        model.fit(X, y)
        models[var] = model

        # Compute time index for future month
        last_t = df_feat["t"].iloc[-1]
        last_year = df_feat["year"].iloc[-1]
        last_month = df_feat["month"].iloc[-1]
        delta_months = (year - last_year) * 12 + (month - last_month)
        future_t = last_t + delta_months

        X_future = np.array([[future_t, month]])
        pred_val = float(model.predict(X_future)[0])

        # Round to two decimal places
        predictions[var] = round(pred_val, 2)

    return predictions, models, df_feat

# --- 1) Load & merge the two export CSVs ---
def load_merged_exports(paths):
    dfs = []
    for p in paths:
        df = pd.read_csv(p)
        # Ensure expected column names exist
        needed = {'date','T2M_C','QV2M_gkg','U2M','V2M','PRECTOT_mm','longitude','latitude'}
        missing = needed.difference(df.columns)
        if missing:
            raise ValueError(f"{p} is missing columns: {sorted(missing)}")

        # Parse date: your sample is like "1/1/23"
        # If you ever change to ISO (YYYY-MM-01), this still works.
        df['date'] = pd.to_datetime(df['date'], errors='raise', infer_datetime_format=True)

        # Coerce numerics just in case CSV has stray strings
        for col in ['T2M_C','QV2M_gkg','U2M','V2M','PRECTOT_mm','longitude','latitude']:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        dfs.append(df)

    full = pd.concat(dfs, ignore_index=True).dropna(subset=['date','longitude','latitude'])
    # Sort for stable time indices
    full = full.sort_values(['latitude','longitude','date']).reset_index(drop=True)
    return full

# --- 2) Find nearest native grid cell (haversine for robustness) ---
def nearest_cell_coords(df_all, lat, lon):
    # Use unique cell centers to avoid scanning every row-month
    pts = df_all[['latitude','longitude']].drop_duplicates().to_numpy()

    # Haversine distance (km)
    lat1, lon1 = radians(lat), radians(lon)
    def hav(p):
        lat2, lon2 = radians(p[0]), radians(p[1])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2
        return 2*asin(sqrt(a))
    # Vectorized-ish min search
    dists = np.array([hav(p) for p in pts])
    iy = int(np.argmin(dists))
    return float(pts[iy][0]), float(pts[iy][1])  # (lat_cell, lon_cell)

# --- 3) Build a per-cell dataframe in the shape your model expects ---
def build_cell_df(df_all, lat_cell, lon_cell):
    sub = df_all[(df_all['latitude']==lat_cell) & (df_all['longitude']==lon_cell)].copy()
    # Keep only columns needed by your model
    keep = ['date','T2M_C','QV2M_gkg','U2M','V2M','PRECTOT_mm']
    sub = sub[keep].sort_values('date').reset_index(drop=True)
    return sub

# --- 4) One-call helper for your front end/backend ---
def predict_from_csvs(user_lat, user_lon, target_date, csv_paths):
    """
    user_lat, user_lon : float
    target_date        : datetime (future date; only year & month are used by your model)
    csv_paths          : list like ['exports/merra2_na_2023.csv','exports/merra2_na_2024.csv']
    """
    # Load both years
    full = load_merged_exports(csv_paths)

    # Snap to nearest native grid cell
    lat_cell, lon_cell = nearest_cell_coords(full, user_lat, user_lon)

    # Build the cell-level monthly dataframe
    cell_df = build_cell_df(full, lat_cell, lon_cell)

    # Call YOUR existing function (unchanged)
    preds, models, df_feat = train_and_predict_future(cell_df, target_date.year, target_date.month)

    return {
        "nearest_cell": {"latitude": lat_cell, "longitude": lon_cell},
        "n_months": int(len(cell_df)),
        "train_window": {
            "start": str(cell_df['date'].min().date()) if not cell_df.empty else None,
            "end":   str(cell_df['date'].max().date()) if not cell_df.empty else None
        },
        "predictions": preds,   # already rounded to 2 decimals by your function
        "df_used": df_feat      # optional: features your model trained on
    }

result = predict_from_csvs(
    user_lat=40.7484,
    user_lon=-73.9857,
    target_date=datetime(2026, 7, 1),
    csv_paths=["merra2_na_2023.csv", "merra2_na_2024.csv"]
)

preds = dict(result["predictions"])
if "PRECTOT_mm" in preds:
    preds["PRECTOT_mm_daily"] = round(preds["PRECTOT_mm"] / 30.0, 2)

# overwrite (optional; or just print preds)
result["predictions"] = preds

## Health Risk ##
def _bin_temp(temp_c: float) -> int:
    # 0=Low, 1=Medium, 2=High
    if temp_c < 21:
        return 0
    elif temp_c <= 32:
        return 1
    else:
        return 2

def _bin_qv(qv_gkg: float) -> int:
    # 0=Low, 1=Medium, 2=High
    if qv_gkg < 7:
        return 0
    elif qv_gkg <= 10:
        return 1
    else:
        return 2

def health_risk_minbin(temp_c: float, qv_gkg: float) -> str:
    """
    Overall risk is the MIN of the two bins (Low<Med<High).
    - If either is Low → Low
    - If one is High and the other Medium → Medium
    - Only High when both are High
    """
    t_bin = _bin_temp(temp_c)
    q_bin = _bin_qv(qv_gkg)
    risk_bin = min(t_bin, q_bin)
    return ["Low", "Medium", "High"][risk_bin]

if "T2M_C" in preds and "QV2M_gkg" in preds:
    preds["Health_Risk"] = health_risk_minbin(preds["T2M_C"], preds["QV2M_gkg"])
## Chance of being swept away ##
import math

def swept_away_risk(u_ms: float, v_ms: float) -> str:
    """
    Compute 'Chance of Being Swept Away' category from U/V wind components (m/s).

    Returns: 'Low', 'Medium', or 'High'
      - Low    : total wind speed < 20 m/s
      - Medium : 20 ≤ total < 40 m/s
      - High   : total ≥ 40 m/s
    """
    # Compute total wind speed magnitude
    ws = math.hypot(u_ms, v_ms)  # sqrt(U^2 + V^2)

    if ws >= 40:
        return "High"
    elif ws >= 20:
        return "Medium"
    else:
        return "Low"

# Add Chance of Being Swept Away
if "U2M" in preds and "V2M" in preds:
    preds["Swept_Away_Risk"] = swept_away_risk(preds["U2M"], preds["V2M"])


print(result["nearest_cell"])
print(preds)
